---
layout: post
title: "Bitmasking"
---

During this morning’s showerthoughts I was trying to think of a way of doing
ipv6 subnet contains checking quickly. I ended up using the compiler’s asm
output to guide that design and figured I’d write something up about the
process

In ipv4, subnet contains checking is easy since everything fits in a word,
roughly:

```c
// checking if 1.2.3.0/8 contains 1.2.3.4
uint32_t prefix = 0x01020300; // prefix address, packed big endian
uint32_t client = 0x01020304; // client address, packed big endian
uint8_t mask = 8; // netmask, range 0-32
uint32_t bitmask = 0xFFFFFFFF << (32 - mask); // invert the mask to get a count of number of zeros
if ( (prefix & bitmask) == (client & bitmask) ) {
    // subnet contains client
}
```

IPv6 addresses are 128-bits long, which means we need to juggle two machine
words. In computing the bitmask we need a mask for the upper and the lower
portion of the address.

```c
uint8_t mask = 66; // netmask, range 0-128
uint64_t upper_bitmask = -1;
uint64_t lower_bitmask = -1;
if (mask < 64) {
    lower_bitmask <<= mask;
} else {
    upper_bitmask = lower_bitmask << (64 - mask);
    lower = 0;
}
```

I didn’t like the branch in there, so wanted to make sure it was absolutely necessary. 

gcc/clang/etc. have an int128 type; I dumped the assembly to make sure I
wasn’t overlooking a clever way of avoiding the branch. I was hoping/expecting
for some magical SSE instruction that did it in one opcode, but that wasn’t the
case:

Unoptimized assembly output https://godbolt.org/z/JWmZuC


```
        mov     QWORD PTR [rbp-16], -1 ; set all 1s in least significant word
        mov     QWORD PTR [rbp-8], -1 ; set all 1s in most significant word
        movzx   eax, BYTE PTR [rbp-17] ; read byte into accumulator register
        movzx   ecx, al ; move low byte of accumulator to the counter register
        mov     rax, QWORD PTR [rbp-16] ; copy least significant word into the accumulator register so we can use it
        mov     rdx, QWORD PTR [rbp-8] ; copy most significant word into the data register so we can use it
        shld    rdx, rax, cl ; 'double precision shift left', shift cl number of bits from rax into rdx, rax is unchanged
        sal     rax, cl ; now shift rax by cl bits
        test    cl, 64 ; check if our shift count was <= 64, if so jump to L3
        je      .L3
        mov     rdx, rax ; we shifted by > 64 bits, so swap the least significant word into most significant word
        xor     eax, eax ; and zero out the least significant word
.L3:
        mov     QWORD PTR [rbp-16], rax ; write back our results from the registers
        mov     QWORD PTR [rbp-8], rdx
        mov     eax, 0 ; return 0, interesting that this is a mov and not xor
        leave
        ret
```

Interesting notes:
There’s still a branch in there, so my original code isn’t off track. That confirmation was the main thing I wanted.
xor eax, eax clears the lower and upper 32-bits. I’m fairly new to x86_64 asm so was expecting xor rax, rax. If you want to go down this particular rabbit hole, look for info on REX prefixes and partial register renaming.
shld is an interesting instruction that I hadn’t seen before, but it’s overkill for our use case - since we’re always shifting in zeros, a shl is sufficient. The optimized build recognizes this (https://godbolt.org/z/j4nQf2)

Looking at the assembly, I tried to think of a way of hitting that exact same codegen via other C++ constructs. The closest seemed like std::bitset if it had optimizations for sizes up to 128bits, but that didn’t pan out - std::bitset is general purpose and produces an enormous pile of code for what we’re trying to do (https://godbolt.org/z/XgyEVU).

Comparing the original ipv6 C++ pseudocode (https://godbolt.org/z/ReJV4p) to the optimized int128 version (https://godbolt.org/z/j4nQf2), things look really good. 

The int128 version is a tad tighter because the sal instruction automatically limits its shift operand. Unfortunately in the C++ spec it’s undefined behavior to shift by a negative operand or an operand greater than/equal to the bit length of what we’re shifting, so it’s difficult to take advantage of that behavior.

Todo: `_mm_slli_si128`
